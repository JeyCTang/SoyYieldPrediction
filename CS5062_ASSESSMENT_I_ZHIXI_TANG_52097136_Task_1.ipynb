{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "Assessment_Task_1.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "v8Yo33NA4BRJ",
        "D31GG0Iy9aP6",
        "NTHA8nQ09aP_",
        "LV4hAV0Y9aQD",
        "Y00VNhpv3ibM",
        "_o1TXXPcIGO-"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "WOH8sXv49aP4"
      },
      "source": [
        "# CS5062_ASSESSMENT1_ZHIXI_TANG__52097136\n",
        "\n",
        "Student: ZHIXI TANG\n",
        "Student ID: 52097136\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This .ipynb file include the source code and detailed explanation of CS5062 Assessment 1. It can be run on local machine with\n",
        "required environment or Google Colab.\n",
        "\n",
        "**Run on local machine:**\n",
        "\n",
        "* System:\n",
        "* Python Version:\n",
        "* IDE: Pycharm:\n",
        "* Required packages: see `requirements.txt` or use command `$ pip install -r requirements.txt`\n",
        "\n",
        "**Run on Colab:**\n",
        "\n",
        "Simply uploading this .ipynb file on Google Colab then feel free to play it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8Yo33NA4BRJ"
      },
      "source": [
        "## Task 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "D31GG0Iy9aP6"
      },
      "source": [
        "### Task 1 - A: Data Import\n",
        "\n",
        "We will use `pandas` to read the .csv data file as the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "8Aey5RkV9aP6",
        "outputId": "3c994041-0575-4256-c6e7-6b04e6db3582"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "df = pd.read_csv('./data/soybean_tabular.csv')\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variety</th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>M_1</th>\n",
              "      <th>M_2</th>\n",
              "      <th>M_3</th>\n",
              "      <th>W_1</th>\n",
              "      <th>W_2</th>\n",
              "      <th>W_3</th>\n",
              "      <th>W_4</th>\n",
              "      <th>Yield</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>4.0900</td>\n",
              "      <td>15.3</td>\n",
              "      <td>396.90</td>\n",
              "      <td>4.98</td>\n",
              "      <td>0.00632</td>\n",
              "      <td>18.0</td>\n",
              "      <td>6.575</td>\n",
              "      <td>2.381979</td>\n",
              "      <td>0.475522</td>\n",
              "      <td>65.2</td>\n",
              "      <td>296.350195</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>17.8</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.14</td>\n",
              "      <td>0.02731</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.421</td>\n",
              "      <td>7.071148</td>\n",
              "      <td>0.509165</td>\n",
              "      <td>78.9</td>\n",
              "      <td>241.620198</td>\n",
              "      <td>21.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>4.9671</td>\n",
              "      <td>17.8</td>\n",
              "      <td>392.83</td>\n",
              "      <td>4.03</td>\n",
              "      <td>0.02729</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.185</td>\n",
              "      <td>6.896941</td>\n",
              "      <td>0.580673</td>\n",
              "      <td>61.1</td>\n",
              "      <td>241.551476</td>\n",
              "      <td>34.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>18.7</td>\n",
              "      <td>394.63</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.03237</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.998</td>\n",
              "      <td>2.237817</td>\n",
              "      <td>0.491539</td>\n",
              "      <td>45.8</td>\n",
              "      <td>222.023994</td>\n",
              "      <td>33.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3</td>\n",
              "      <td>6.0622</td>\n",
              "      <td>18.7</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.33</td>\n",
              "      <td>0.06905</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.147</td>\n",
              "      <td>1.979327</td>\n",
              "      <td>0.103660</td>\n",
              "      <td>54.2</td>\n",
              "      <td>221.723972</td>\n",
              "      <td>36.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>501</th>\n",
              "      <td>1</td>\n",
              "      <td>2.4786</td>\n",
              "      <td>21.0</td>\n",
              "      <td>391.99</td>\n",
              "      <td>9.67</td>\n",
              "      <td>0.06263</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.593</td>\n",
              "      <td>11.774062</td>\n",
              "      <td>0.565469</td>\n",
              "      <td>69.1</td>\n",
              "      <td>272.952382</td>\n",
              "      <td>22.4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>502</th>\n",
              "      <td>1</td>\n",
              "      <td>2.2875</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>9.08</td>\n",
              "      <td>0.04527</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.120</td>\n",
              "      <td>11.785249</td>\n",
              "      <td>0.562102</td>\n",
              "      <td>76.7</td>\n",
              "      <td>273.264306</td>\n",
              "      <td>20.6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>503</th>\n",
              "      <td>1</td>\n",
              "      <td>2.1675</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>5.64</td>\n",
              "      <td>0.06076</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.976</td>\n",
              "      <td>11.854205</td>\n",
              "      <td>0.777569</td>\n",
              "      <td>91.0</td>\n",
              "      <td>273.421726</td>\n",
              "      <td>23.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>504</th>\n",
              "      <td>1</td>\n",
              "      <td>2.3889</td>\n",
              "      <td>21.0</td>\n",
              "      <td>393.45</td>\n",
              "      <td>6.48</td>\n",
              "      <td>0.10959</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.794</td>\n",
              "      <td>11.765196</td>\n",
              "      <td>0.679057</td>\n",
              "      <td>89.3</td>\n",
              "      <td>273.913622</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>505</th>\n",
              "      <td>1</td>\n",
              "      <td>2.5050</td>\n",
              "      <td>21.0</td>\n",
              "      <td>396.90</td>\n",
              "      <td>7.88</td>\n",
              "      <td>0.04741</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.030</td>\n",
              "      <td>12.007563</td>\n",
              "      <td>0.615699</td>\n",
              "      <td>80.8</td>\n",
              "      <td>273.623418</td>\n",
              "      <td>11.9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>506 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Variety     S_1   S_2     S_3  ...       W_2   W_3         W_4  Yield\n",
              "0          1  4.0900  15.3  396.90  ...  0.475522  65.2  296.350195   24.0\n",
              "1          2  4.9671  17.8  396.90  ...  0.509165  78.9  241.620198   21.6\n",
              "2          2  4.9671  17.8  392.83  ...  0.580673  61.1  241.551476   34.7\n",
              "3          3  6.0622  18.7  394.63  ...  0.491539  45.8  222.023994   33.4\n",
              "4          3  6.0622  18.7  396.90  ...  0.103660  54.2  221.723972   36.2\n",
              "..       ...     ...   ...     ...  ...       ...   ...         ...    ...\n",
              "501        1  2.4786  21.0  391.99  ...  0.565469  69.1  272.952382   22.4\n",
              "502        1  2.2875  21.0  396.90  ...  0.562102  76.7  273.264306   20.6\n",
              "503        1  2.1675  21.0  396.90  ...  0.777569  91.0  273.421726   23.9\n",
              "504        1  2.3889  21.0  393.45  ...  0.679057  89.3  273.913622   22.0\n",
              "505        1  2.5050  21.0  396.90  ...  0.615699  80.8  273.623418   11.9\n",
              "\n",
              "[506 rows x 13 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "SYdZFgcJ9aP8"
      },
      "source": [
        "Similarly, we use `pandas` to get the statistical summary infomation (mean, range, standard deviations, min/max, median, and 25%/50%/75% percentile)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "GbFNtcHW9aP8",
        "outputId": "2fc1078d-8e47-4a5d-d6e7-31cafca3442a"
      },
      "source": [
        "df1 = df.iloc[:, 1:]\n",
        "\n",
        "# range\n",
        "df1.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>S_1</th>\n",
              "      <th>S_2</th>\n",
              "      <th>S_3</th>\n",
              "      <th>S_4</th>\n",
              "      <th>M_1</th>\n",
              "      <th>M_2</th>\n",
              "      <th>M_3</th>\n",
              "      <th>W_1</th>\n",
              "      <th>W_2</th>\n",
              "      <th>W_3</th>\n",
              "      <th>W_4</th>\n",
              "      <th>Yield</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "      <td>506.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.795043</td>\n",
              "      <td>18.455534</td>\n",
              "      <td>356.674032</td>\n",
              "      <td>12.653063</td>\n",
              "      <td>3.613524</td>\n",
              "      <td>11.363636</td>\n",
              "      <td>6.284634</td>\n",
              "      <td>11.131488</td>\n",
              "      <td>0.551258</td>\n",
              "      <td>68.574901</td>\n",
              "      <td>408.252839</td>\n",
              "      <td>22.532806</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.105710</td>\n",
              "      <td>2.164946</td>\n",
              "      <td>91.294864</td>\n",
              "      <td>7.141062</td>\n",
              "      <td>8.601545</td>\n",
              "      <td>23.322453</td>\n",
              "      <td>0.702617</td>\n",
              "      <td>6.868654</td>\n",
              "      <td>0.181693</td>\n",
              "      <td>28.148861</td>\n",
              "      <td>168.530578</td>\n",
              "      <td>9.197104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.129600</td>\n",
              "      <td>12.600000</td>\n",
              "      <td>0.320000</td>\n",
              "      <td>1.730000</td>\n",
              "      <td>0.006320</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.561000</td>\n",
              "      <td>0.472905</td>\n",
              "      <td>0.103660</td>\n",
              "      <td>2.900000</td>\n",
              "      <td>186.765075</td>\n",
              "      <td>5.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.100175</td>\n",
              "      <td>17.400000</td>\n",
              "      <td>375.377500</td>\n",
              "      <td>6.950000</td>\n",
              "      <td>0.082045</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.885500</td>\n",
              "      <td>5.149096</td>\n",
              "      <td>0.422286</td>\n",
              "      <td>45.025000</td>\n",
              "      <td>278.745884</td>\n",
              "      <td>17.025000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>3.207450</td>\n",
              "      <td>19.050000</td>\n",
              "      <td>391.440000</td>\n",
              "      <td>11.360000</td>\n",
              "      <td>0.256510</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.208500</td>\n",
              "      <td>9.588040</td>\n",
              "      <td>0.528133</td>\n",
              "      <td>77.500000</td>\n",
              "      <td>330.467783</td>\n",
              "      <td>21.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>5.188425</td>\n",
              "      <td>20.200000</td>\n",
              "      <td>396.225000</td>\n",
              "      <td>16.955000</td>\n",
              "      <td>3.677082</td>\n",
              "      <td>12.500000</td>\n",
              "      <td>6.623500</td>\n",
              "      <td>18.094315</td>\n",
              "      <td>0.668573</td>\n",
              "      <td>94.075000</td>\n",
              "      <td>665.354401</td>\n",
              "      <td>25.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>12.126500</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>396.900000</td>\n",
              "      <td>37.970000</td>\n",
              "      <td>88.976200</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>8.780000</td>\n",
              "      <td>28.074197</td>\n",
              "      <td>1.158538</td>\n",
              "      <td>100.000000</td>\n",
              "      <td>711.210992</td>\n",
              "      <td>50.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              S_1         S_2         S_3  ...         W_3         W_4       Yield\n",
              "count  506.000000  506.000000  506.000000  ...  506.000000  506.000000  506.000000\n",
              "mean     3.795043   18.455534  356.674032  ...   68.574901  408.252839   22.532806\n",
              "std      2.105710    2.164946   91.294864  ...   28.148861  168.530578    9.197104\n",
              "min      1.129600   12.600000    0.320000  ...    2.900000  186.765075    5.000000\n",
              "25%      2.100175   17.400000  375.377500  ...   45.025000  278.745884   17.025000\n",
              "50%      3.207450   19.050000  391.440000  ...   77.500000  330.467783   21.200000\n",
              "75%      5.188425   20.200000  396.225000  ...   94.075000  665.354401   25.000000\n",
              "max     12.126500   22.000000  396.900000  ...  100.000000  711.210992   50.000000\n",
              "\n",
              "[8 rows x 12 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "KsoDkW7g9aP9"
      },
      "source": [
        "The statistical description is shown above. As the feature `variety` is just a number to represent corp variety, we do not\n",
        "have to count its means, std, min, etc.\n",
        "\n",
        "From the above data we can see in the column `M_2`, there are some values are missed. The dataset has used $0$ to replace\n",
        "these missed values.\n",
        "\n",
        "However, to a certain variety of crops, there maybe a certain result corresponding to its features and yield,\n",
        "therefore, we will get the statistical description of different varieties of corps separately."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sp6EKXkF9aP9",
        "outputId": "bd7f249a-6b32-4191-8cce-c5ad4b52f426"
      },
      "source": [
        "# count how many varieties in corps\n",
        "df['Variety'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24    132\n",
              "5     115\n",
              "4     110\n",
              "3      38\n",
              "6      26\n",
              "8      24\n",
              "2      24\n",
              "1      20\n",
              "7      17\n",
              "Name: Variety, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "sa7TvcxY9aP9"
      },
      "source": [
        "def show_describe(dataframe, variety_num):\n",
        "    \"\"\"\"The variety_num can only be one of [24, 5, 4, 3, 6, 2, 8, 1, 7]\"\"\"\n",
        "    df_v = dataframe.loc[dataframe['Variety'] == variety_num]\n",
        "    return  df_v.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9p55z9rU9aP-"
      },
      "source": [
        "def DiffYield(dataframe):\n",
        "    \"\"\"Input the table of .csv, return the summary of yeild based on different variety\"\"\"\n",
        "    varieties = [24, 5, 4, 3, 6, 2, 8, 1, 7]\n",
        "    index = ['mean', 'std', 'min', '25%', '50%', '75%', 'max']\n",
        "    v_names = []\n",
        "    data = {}\n",
        "\n",
        "    for i in varieties:\n",
        "        df_v = show_describe(dataframe, i)  # get the describe info of a single variety\n",
        "        corp_yield = df_v.iloc[1:, -1].values\n",
        "        exec(f\"data['Variety_{i}'] = corp_yield\")\n",
        "    yield_df = pd.DataFrame(data, index=index)\n",
        "\n",
        "    return yield_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlETqDYM9aP-",
        "outputId": "76678695-2665-4b11-fcc1-e320e70994e9"
      },
      "source": [
        "# df.iloc[1:, -1]\n",
        "df.iloc[1:, -1].index"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RangeIndex(start=1, stop=506, step=1)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "ZtZ0olWl9aP_",
        "outputId": "75a15a03-a417-4748-9525-42faf698e6c1"
      },
      "source": [
        "DiffYield(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Variety_24</th>\n",
              "      <th>Variety_5</th>\n",
              "      <th>Variety_4</th>\n",
              "      <th>Variety_3</th>\n",
              "      <th>Variety_6</th>\n",
              "      <th>Variety_2</th>\n",
              "      <th>Variety_8</th>\n",
              "      <th>Variety_1</th>\n",
              "      <th>Variety_7</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>16.403788</td>\n",
              "      <td>25.706957</td>\n",
              "      <td>21.387273</td>\n",
              "      <td>27.928947</td>\n",
              "      <td>20.976923</td>\n",
              "      <td>26.833333</td>\n",
              "      <td>30.358333</td>\n",
              "      <td>24.365000</td>\n",
              "      <td>27.105882</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.539745</td>\n",
              "      <td>9.328401</td>\n",
              "      <td>6.957883</td>\n",
              "      <td>8.324692</td>\n",
              "      <td>2.312801</td>\n",
              "      <td>7.874376</td>\n",
              "      <td>9.727724</td>\n",
              "      <td>8.024454</td>\n",
              "      <td>6.493215</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.800000</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>14.400000</td>\n",
              "      <td>16.800000</td>\n",
              "      <td>15.700000</td>\n",
              "      <td>16.000000</td>\n",
              "      <td>11.900000</td>\n",
              "      <td>17.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>11.225000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>17.575000</td>\n",
              "      <td>21.125000</td>\n",
              "      <td>18.900000</td>\n",
              "      <td>21.400000</td>\n",
              "      <td>23.825000</td>\n",
              "      <td>20.475000</td>\n",
              "      <td>24.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>14.400000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>20.450000</td>\n",
              "      <td>26.500000</td>\n",
              "      <td>21.200000</td>\n",
              "      <td>23.850000</td>\n",
              "      <td>28.250000</td>\n",
              "      <td>22.200000</td>\n",
              "      <td>26.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>19.900000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>23.650000</td>\n",
              "      <td>34.525000</td>\n",
              "      <td>23.025000</td>\n",
              "      <td>33.225000</td>\n",
              "      <td>33.175000</td>\n",
              "      <td>27.225000</td>\n",
              "      <td>29.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>50.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>24.800000</td>\n",
              "      <td>43.800000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>50.000000</td>\n",
              "      <td>42.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      Variety_24  Variety_5  Variety_4  ...  Variety_8  Variety_1  Variety_7\n",
              "mean   16.403788  25.706957  21.387273  ...  30.358333  24.365000  27.105882\n",
              "std     8.539745   9.328401   6.957883  ...   9.727724   8.024454   6.493215\n",
              "min     5.000000  11.800000   7.000000  ...  16.000000  11.900000  17.600000\n",
              "25%    11.225000  19.500000  17.575000  ...  23.825000  20.475000  24.300000\n",
              "50%    14.400000  23.000000  20.450000  ...  28.250000  22.200000  26.200000\n",
              "75%    19.900000  30.000000  23.650000  ...  33.175000  27.225000  29.600000\n",
              "max    50.000000  50.000000  50.000000  ...  50.000000  50.000000  42.800000\n",
              "\n",
              "[7 rows x 9 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "c4scxq1k9aP_"
      },
      "source": [
        "From above table we can tell that to different varieties of corps, the yield is also different. For example, The Max yield of\n",
        "`Variety_6` is just 24.8 which is much lower than other varieties. According to the mean of `variety_24`, we can tell its average\n",
        "yield is much lower than other varieties. Therefore, we can conclude that the variety of corps affects its yield."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "NTHA8nQ09aP_"
      },
      "source": [
        "### Task 1 - B: Data pre-processing\n",
        "\n",
        "In this section, we will split dataset to train, validation, test with rate of 6:2:2. At the same time, we will try our\n",
        "best to ensure the fairness and uniformity of data. According to the conclusion of `Task 1 - A`, we know that the variety\n",
        "of corps affects its yield. Therefore, we will take samples from each variety at a ratio of $6:2:2$ (train:validation:test),\n",
        "and then compose the datasets. In this case, we can assure as much as possible that in each set of data, the variety proportions of\n",
        "corps are approximately equal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "1QDen5jb9aQA"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def extract_df_by_variety(dataframe):\n",
        "    \"\"\"Split dataframe based on varieties, return all dataframes with dictionary\"\"\"\n",
        "    varieties = [24, 5, 4, 3, 6, 2, 8, 1, 7]\n",
        "    extracted_df = {}\n",
        "    for variety in varieties:\n",
        "        df_s = dataframe.loc[dataframe['Variety'] == variety]\n",
        "        # exec(f\"extracted_df['Variety_{variety}'] = df_s\")\n",
        "        extracted_df[variety] = df_s\n",
        "    return extracted_df\n",
        "\n",
        "def split_single_dataset(dataframe):\n",
        "    \"\"\"split particular dataset to train:val:test = 6:2:2\"\"\"\n",
        "    train_set = dataframe.sample(frac=0.6, random_state=0, axis=0)\n",
        "    rest_set = dataframe[~dataframe.index.isin(train_set.index)]\n",
        "    test_set = rest_set.sample(frac=0.5, random_state=0, axis=0)\n",
        "    val_set = rest_set[~rest_set.index.isin(test_set.index)]\n",
        "\n",
        "    return train_set, test_set, val_set\n",
        "\n",
        "def stratified_sampling(dataframe):\n",
        "    \"\"\"\"Combine above functions, input the dataframe, return the stratified sampled datasets\"\"\"\n",
        "    extracted_dfs = extract_df_by_variety(dataframe)\n",
        "    train_sets, test_sets, val_sets = [], [], []\n",
        "    for _df in extracted_dfs.values():\n",
        "        train_set, test_set, val_set = split_single_dataset(_df)\n",
        "        train_sets.append(train_set)\n",
        "        test_sets.append(test_set)\n",
        "        val_sets.append(val_set)\n",
        "    p_train = np.array(pd.concat(train_sets).sample(frac=1), dtype='float32')\n",
        "    p_test = np.array(pd.concat(test_sets).sample(frac=1), dtype='float32')\n",
        "    p_val = np.array(pd.concat(val_sets).sample(frac=1), dtype='float32')\n",
        "\n",
        "    return p_train, p_test, p_val\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4FPc0exm9aQA"
      },
      "source": [
        "# get train, validation, test datasets\n",
        "train_set, test_set, val_set = stratified_sampling(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "wUwM3OfD9aQB"
      },
      "source": [
        "def tensor_generator(dataset):\n",
        "    \"\"\"input stratified sampled dataset, return variable x and result y\"\"\"\n",
        "    x = torch.tensor(dataset[...,:12])\n",
        "    y = torch.tensor(dataset[...,12:])\n",
        "    return x, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HLQA9lc9aQB",
        "outputId": "42f6defd-ddaf-44db-b819-57020b864743"
      },
      "source": [
        "# get tensors\n",
        "\n",
        "x_train, y_train = tensor_generator(train_set)\n",
        "x_val, y_val = tensor_generator(val_set)\n",
        "x_test, y_test = tensor_generator(test_set)\n",
        "\n",
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[6.0000e+00, 2.8927e+00, 1.9200e+01,  ..., 4.5895e-01, 7.0600e+01,\n",
              "         3.9106e+02],\n",
              "        [5.0000e+00, 1.4191e+00, 1.4700e+01,  ..., 9.5758e-01, 1.0000e+02,\n",
              "         4.0364e+02],\n",
              "        [2.4000e+01, 1.6475e+00, 2.0200e+01,  ..., 4.9562e-01, 8.9100e+01,\n",
              "         6.6605e+02],\n",
              "        ...,\n",
              "        [2.0000e+00, 1.9929e+00, 1.9100e+01,  ..., 4.5043e-01, 8.8400e+01,\n",
              "         1.8773e+02],\n",
              "        [2.4000e+01, 1.6768e+00, 2.0200e+01,  ..., 6.9845e-01, 9.6000e+01,\n",
              "         6.6531e+02],\n",
              "        [2.0000e+00, 2.1974e+00, 1.9100e+01,  ..., 2.9167e-01, 8.4100e+01,\n",
              "         1.8753e+02]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "mRvVV8fI9aQB"
      },
      "source": [
        "According to the first table shown in `Task 1 - A`, we can tell that the values of different features have observable difference, for example,\n",
        "in the first example, the value of feature `S_3` is 369.90, the value of feature `W_2` is 0.475522. To eliminate the gradient descent majorly\n",
        "depends on some features, we will do z-score normalization to the variable $X$. As we know, the formula of z-score is: $\\hat X = \\frac{X-\\mu}{\\epsilon}$,\n",
        "where $\\hat X$ is the normalized variable $X$, $\\mu$ is the mean of $X$, $\\epsilon$ is the standard deviation of $X$. Therefore, we can define\n",
        "function as the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Vm5z5U589aQC"
      },
      "source": [
        "# def normalization(dataframe, columns):\n",
        "#     sc_set = StandardScaler().fit_transform(dataframe)\n",
        "#     return pd.DataFrame(data=sc_set, columns=columns)\n",
        "def normalization_x(x, y):\n",
        "    \"\"\"Input a tensor, return the z-score normalized tensor\"\"\"\n",
        "    mean = torch.mean(x)\n",
        "    std = torch.std(x)\n",
        "    normed_x = (x - mean) / std\n",
        "    normed_y = (y - mean) / std\n",
        "    return normed_x, normed_y\n",
        "\n",
        "# normalize X\n",
        "normed_x_train, normed_y_train = normalization_x(x_train, y_train)\n",
        "normed_x_val, normed_y_val = normalization_x(x_val, y_val)\n",
        "normed_x_test, normed_y_test = normalization_x(x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "LV4hAV0Y9aQD"
      },
      "source": [
        "### Task 1 - C: Linear Regression Training\n",
        "\n",
        "In this section we will define the Linear models and fit them with PyTorch. According to the requirements of task. We will create two linear models. One is Ridge regression, the other one is Lasso regression. \n",
        "\n",
        "During the traininig process, we will count the mean square error for the training set and validation set. Then we choose the best performance model according to these observed values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aW61QYe9C3PM"
      },
      "source": [
        "In fact, Edge regression and Lasso regression is L2 and L1 optimization in Linear regression. \n",
        "\n",
        "The essence of L1 optimization is adding a $\\frac{1}{2}\\lambda\\omega^2$ to every $\\omega$ of the function($\\frac{1}{2}\\lambda||W||^2 = \\frac{1}{2}\\sum_j \\omega^2_j$). Therefore, to define L1 optimization, we will re-write the calculation process of loss function during train process. \n",
        "\n",
        "To define the Edge regression, L2 regularization means that all $\\omega$ decrease linearly toward 0 with $\\omega += -\\lambda * W$. Fortunately, in PyTorch, the optimizer has parameter `weight_decay(float, optional)`, when this parameter is not equal to 0, it is L2 regularizaiton.\n",
        "\n",
        "Therefore, we can define `edge_linear` and `lasso_linear` to train our model as below. \n",
        "\n",
        "The initial hyperparameters will be set as the following, then we will try to find the optimal hyperparameters after the first trains. \n",
        "\n",
        "* lr(learning rate/step size) = 0.001\n",
        "* epoch = 10000\n",
        "* L1/L2 lambda = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Z0qh7q9X9aQC"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def edge_linear(x_train, y_train, x_val, y_val, epoch, p_step=10, lr=0.01, save_model=False, gpu=False, vis=False, lambda_L=0):\n",
        "    \"\"\"Edge regression\"\"\"\n",
        "\n",
        "    model = torch.nn.Linear(12, 1, bias=True)  # define Linear model\n",
        "    loss_func = torch.nn.MSELoss()  # define loss function\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=lambda_L)  # define optimizer\n",
        "    loss_history = []\n",
        "    loss_val_history = []\n",
        "    if gpu:\n",
        "        model = model.cuda(0)\n",
        "        x_train = x_train.cuda(0)\n",
        "        y_train = y_train.cuda(0)\n",
        "        x_val = x_val.cuda(0)\n",
        "        y_val = y_val.cuda(0)\n",
        "\n",
        "    print('iter,\\ttrain_loss,\\tval_loss')\n",
        "\n",
        "    for i in range(epoch):\n",
        "        \"\"\"Train process\"\"\"\n",
        "        y_hat = model(x_train)\n",
        "        loss = loss_func(y_hat, y_train)\n",
        "        loss_history.append(loss)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        # print(model.weight.detach().numpy())\n",
        "        \"\"\"validation process\"\"\"\n",
        "        y_val_hat = model(x_val)\n",
        "        loss_val = loss_func(y_val_hat, y_val)\n",
        "        loss_val_history.append(loss_val)\n",
        "        # print(model.weight.detach().numpy())\n",
        "        \"\"\"print the train loss and validation loss\"\"\"\n",
        "        if i % p_step == 0 or i==epoch-1:\n",
        "            print(f'{i}\\t{loss.item():.4f}\\t\\t{loss_val.item():.4f}')\n",
        "    if save_model:\n",
        "        torch.save(model, './EdgeLinear.pth')\n",
        "    if vis:\n",
        "        x_1 = loss_history\n",
        "        x_2 = loss_val_history\n",
        "        y = range(epoch)\n",
        "        plt.plot(y, x_1, label=\"train loss\")\n",
        "        plt.plot(y, x_2, label=\"val loss\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(\"loss value\")\n",
        "        plt.title(\"Loss Values\")\n",
        "        plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-ZWxAxD49aQE"
      },
      "source": [
        "def lasso_linear(x_train, y_train, x_val, y_val, epoch, p_step=10, lr=0.001, save_model=False, gpu=False, vis=False, lambda_L=0):\n",
        "    \"\"\"Lasso regression\"\"\"\n",
        "    model = torch.nn.Linear(12, 1, bias=True)  # define Linear model\n",
        "    loss_func = torch.nn.MSELoss()  # define loss function\n",
        "    optim = torch.optim.SGD(model.parameters(), lr=lr)  # define optimizer\n",
        "    loss_history = []\n",
        "    loss_val_history = []\n",
        "    if gpu:\n",
        "        model = model.cuda(0)\n",
        "        x_train = x_train.cuda(0)\n",
        "        y_train = y_train.cuda(0)\n",
        "        x_val = x_val.cuda(0)\n",
        "        y_val = y_val.cuda(0)\n",
        "    print('iter,\\ttrain_loss,\\tval_loss')\n",
        "\n",
        "    for i in range(epoch):\n",
        "        \"\"\"Train process\"\"\"\n",
        "        w = 0\n",
        "        for param in model.parameters():\n",
        "            w += torch.sum(abs(param))\n",
        "        y_hat = model(x_train)\n",
        "        loss = loss_func(y_hat, y_train) + lambda_L * w\n",
        "        loss_history.append(loss.item())\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "        # print(model.weight.detach().numpy())\n",
        "        \"\"\"validation process\"\"\"\n",
        "        y_val_hat = model(x_val)\n",
        "        loss_val = loss_func(y_val_hat, y_val)\n",
        "        loss_val_history.append(loss_val.item())\n",
        "        # print(model.weight.detach().numpy())\n",
        "        \"\"\"print the train loss and validation loss\"\"\"\n",
        "        if i % p_step == 0 or i==epoch-1:\n",
        "            print(f'{i}\\t{loss.item():.4f}\\t\\t{loss_val.item():.4f}')\n",
        "    if save_model:\n",
        "        torch.save(model, './LassoLinear.pth')\n",
        "    if vis:\n",
        "        x_1 = loss_history\n",
        "        x_2 = loss_val_history\n",
        "        y = range(epoch)\n",
        "        plt.plot(y, x_1, label=\"train loss\")\n",
        "        plt.plot(y, x_2, label=\"val loss\")\n",
        "        plt.xlabel(\"epoch\")\n",
        "        plt.ylabel(\"loss value\")\n",
        "        plt.title(\"Loss Values\")\n",
        "        plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "mkdNQg4uS5Me",
        "outputId": "61f972b4-9574-49e6-8aa7-eac49c99f2b2"
      },
      "source": [
        "edge_linear(normed_x_train, normed_y_train, normed_x_val, normed_y_val, epoch=10000, p_step=500, lr=0.001, save_model=True, gpu=True, vis=True, lambda_L=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter,\ttrain_loss,\tval_loss\n",
            "0\t0.9173\t\t0.8430\n",
            "500\t0.0229\t\t0.0235\n",
            "1000\t0.0095\t\t0.0108\n",
            "1500\t0.0075\t\t0.0090\n",
            "2000\t0.0069\t\t0.0083\n",
            "2500\t0.0064\t\t0.0078\n",
            "3000\t0.0060\t\t0.0073\n",
            "3500\t0.0057\t\t0.0068\n",
            "4000\t0.0054\t\t0.0065\n",
            "4500\t0.0051\t\t0.0061\n",
            "5000\t0.0049\t\t0.0058\n",
            "5500\t0.0047\t\t0.0056\n",
            "6000\t0.0045\t\t0.0053\n",
            "6500\t0.0043\t\t0.0051\n",
            "7000\t0.0042\t\t0.0050\n",
            "7500\t0.0041\t\t0.0048\n",
            "8000\t0.0040\t\t0.0047\n",
            "8500\t0.0039\t\t0.0045\n",
            "9000\t0.0038\t\t0.0044\n",
            "9500\t0.0037\t\t0.0043\n",
            "9999\t0.0036\t\t0.0042\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "SQpXap2d1tlf",
        "outputId": "3e4b4eda-f381-413d-cc55-fbcf8d8c7cb9"
      },
      "source": [
        "lasso_linear(normed_x_train, normed_y_train, normed_x_val, normed_y_val, epoch=10000, p_step=500, lr=0.001, save_model=True, gpu=True, vis=True, lambda_L=0.001)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter,\ttrain_loss,\tval_loss\n",
            "0\t1.2349\t\t1.1668\n",
            "500\t0.0070\t\t0.0072\n",
            "1000\t0.0065\t\t0.0066\n",
            "1500\t0.0063\t\t0.0062\n",
            "2000\t0.0060\t\t0.0059\n",
            "2500\t0.0058\t\t0.0056\n",
            "3000\t0.0056\t\t0.0054\n",
            "3500\t0.0054\t\t0.0051\n",
            "4000\t0.0053\t\t0.0050\n",
            "4500\t0.0052\t\t0.0048\n",
            "5000\t0.0050\t\t0.0047\n",
            "5500\t0.0049\t\t0.0045\n",
            "6000\t0.0048\t\t0.0044\n",
            "6500\t0.0047\t\t0.0043\n",
            "7000\t0.0047\t\t0.0042\n",
            "7500\t0.0046\t\t0.0041\n",
            "8000\t0.0045\t\t0.0040\n",
            "8500\t0.0044\t\t0.0040\n",
            "9000\t0.0044\t\t0.0039\n",
            "9500\t0.0043\t\t0.0038\n",
            "9999\t0.0043\t\t0.0038\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3de5wcZZ3v8c+3J5OEQAghiYAESFg4SEIgwISNRgguylVBX4jAAQMqsLvH68EXaxBFYPW1IK6yaFiIuyAgt8hljRo3igLBPYAEDBAuWcJtmQhmEklMCIHM9O/8UdWdnp6eSWeSSs9Mfd+vVzNVT91+1RXmN8/zVD2liMDMzPKr0OgAzMyssZwIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwKxBJN0v6ZxGx2HmRGADgqSXJX1wGx9zpqQFNcpHS3pH0gHbMh6z3nIiMOu9HwPvkzS+qvw04KmIWNyAmMw2mxOBDWiShki6StIf089Vkoaky0ZL+rmkVZL+LOlBSYV02VckLZO0RtISSUdV7zsiWoHfAp+sWjQDuEnSyHT/bZLeSKfHdhPnJZJ+XDE/TlJIGpTOj5D075JeS+P6pqSmdNk+kh6QtFrSCkl3bI3vzvLDicAGuouAqcBk4CDgMOBr6bIvA63AGGAX4KtASNoP+BwwJSKGA8cAL3ez/xupSATptpOBW0n+/7oB2AvYE3gL+EEvz+NHQDuwD3AwcDRQ6l/4R+BXwEhgLPD9Xh7DcsqJwAa6M4DLImJ5RLQBl7LxF/cGYDdgr4jYEBEPRjL4VgcwBJggqTkiXo6IF7rZ/z3ALpLel87PAH4ZEW0RsTIi7oqIdRGxBvgWMH1zT0DSLsDxwJci4s2IWA58j6QJqnQeewHvjoj1EfG7zT2G5ZsTgQ107wZeqZh/JS0DuBJYCvxK0ouSZgJExFLgS8AlwHJJt0t6NzVExDrgJ8AMSSJJPDcBSBom6TpJr0j6C7AA2KnUpLMZ9gKagdfSZqxVwHXAu9Ll/wAI+L2kpyV9ejP3bznnRGAD3R9JfpGW7JmWERFrIuLLEbE3cCJwfqkvICJujYj3p9sGcEUPx7gR+ATwIWA48LO0/MvAfsBfR8SOwBFpuWrs401gWMX8rhXTrwJvA6MjYqf0s2NETExjfT0izo2IdwN/C1wjaZ8e4jXrxInABpJmSUMrPoOA24CvSRojaTRwMcndPkj6cNrRKmA1SZNQUdJ+kv4m7VReT9K2X+zhuA8Cq4DZwO0R8U5aPjzddpWknYFv9LCPRcARkvaUNAK4sLQgIl4j6QP4Z0k7SipI+itJ09PzOKWiE/oNksTVU7xmnTgR2EAyj+QXb+lzCfBNYCHwJPAU8HhaBrAvcC+wFngIuCYi7iPpH7gcWAG8TtIEU/7FXC3tV7iJpPZwU8Wiq4Dt0v08DPxnD/v4NXBHGudjwM+rVpkBDAaeIfllfydJ/wbAFOARSWuBucAXI+LF7o5lVk1+MY2ZWb65RmBmlnNOBGZmOedEYGaWc04EZmY5N6jRAWyu0aNHx7hx4xodhplZv/LYY4+tiIgxtZb1u0Qwbtw4Fi5c2OgwzMz6FUmvdLfMTUNmZjnnRGBmlnNOBGZmOdfv+gjMbODasGEDra2trF+/vtGh9FtDhw5l7NixNDc3172NE4GZ9Rmtra0MHz6ccePGkYwFaJsjIli5ciWtra2MH1/9BtXuuWnIzPqM9evXM2rUKCeBXpLEqFGjNrtG5URgZn2Kk8CW6c33l5tEsOT1NXz3V0tYsfbtRodiZtan5CYRPL98DVf/dil/fvOdTa9sZrm0atUqrrnmml5te/zxx7Nq1aq617/kkkv4zne+06tjbW25SQRK3w7o1y+YWXd6SgTt7e09bjtv3jx22mmnLMLKXH4SQdpsFjgTmFltM2fO5IUXXmDy5MlccMEF3H///Rx++OGceOKJTJgwAYCPfvSjHHrooUycOJHZs2eXtx03bhwrVqzg5ZdfZv/99+fcc89l4sSJHH300bz11ls9HnfRokVMnTqVAw88kI997GO88cYbAFx99dVMmDCBAw88kNNOOw2ABx54gMmTJzN58mQOPvhg1qxZs8Xnndnto5KuBz4MLI+IA2osPwP4CsmLvNcAfx8RT2QVT6GUCJwHzPqFS3/2NM/88S9bdZ8T3r0j3/jIxG6XX3755SxevJhFixYBcP/99/P444+zePHi8u2Y119/PTvvvDNvvfUWU6ZM4eSTT2bUqFGd9vP8889z22238cMf/pBPfOIT3HXXXZx55pndHnfGjBl8//vfZ/r06Vx88cVceumlXHXVVVx++eW89NJLDBkypNzs9J3vfIdZs2Yxbdo01q5dy9ChQ7f0a8m0RvAj4Ngelr8ETI+IScA/krz4O0NJJig6E5jZZjjssMM63ZN/9dVXc9BBBzF16lReffVVnn/++S7bjB8/nsmTJwNw6KGH8vLLL3e7/9WrV7Nq1SqmT58OwFlnncWCBQsAOPDAAznjjDP48Y9/zKBByd/t06ZN4/zzz+fqq69m1apV5fItkVmNICIWSBrXw/L/VzH7MDA2q1igomnIecCsX+jpL/dtafvtty9P33///dx777089NBDDBs2jCOPPLLmPftDhgwpTzc1NW2yaag7v/jFL1iwYAE/+9nP+Na3vsVTTz3FzJkzOeGEE5g3bx7Tpk1j/vz5vOc97+nV/kv6Sh/BZ4BfdrdQ0nmSFkpa2NbW1qsD+M5kM9uU4cOH99jmvnr1akaOHMmwYcN47rnnePjhh7f4mCNGjGDkyJE8+OCDANx8881Mnz6dYrHIq6++ygc+8AGuuOIKVq9ezdq1a3nhhReYNGkSX/nKV5gyZQrPPffcFsfQ8CEmJH2AJBG8v7t1ImI2adNRS0tLr/6mLz1k4RqBmXVn1KhRTJs2jQMOOIDjjjuOE044odPyY489lmuvvZb999+f/fbbj6lTp26V495444383d/9HevWrWPvvffmhhtuoKOjgzPPPJPVq1cTEXzhC19gp5124utf/zr33XcfhUKBiRMnctxxx23x8RUZ/mZMm4Z+XquzOF1+IHAPcFxE/Hc9+2xpaYnevJjm3mf+xDk3LWTu56Zx4Nj+eYuX2UD37LPPsv/++zc6jH6v1vco6bGIaKm1fsOahiTtCdwNfLLeJLBlx0t+ukZgZtZZlreP3gYcCYyW1Ap8A2gGiIhrgYuBUcA1abNNe3fZamvY8Y1nuGzQDTSt2wdwjcDMrCTLu4ZO38Tyc4Bzsjp+te3efIUZg37NM+sv2FaHNDPrF/rKXUOZ2zjEhNuGzMwq5SYRoORUFR0NDsTMrG/JUSJwjcDMrJbcJAI/R2BmWdhhhx02q7wvyk0iKD9b7ExgZtZJbhKB0j6CiGKDIzGzvmrmzJnMmjWrPF96eczatWs56qijOOSQQ5g0aRI//elP695nRHDBBRdwwAEHMGnSJO644w4AXnvtNY444ggmT57MAQccwIMPPkhHRwdnn312ed3vfe97W/0ca2n4EBPbTLlpyInArF/45Ux4/amtu89dJ8Fxl3e7+NRTT+VLX/oSn/3sZwGYM2cO8+fPZ+jQodxzzz3suOOOrFixgqlTp3LiiSfW9X7gu+++m0WLFvHEE0+wYsUKpkyZwhFHHMGtt97KMcccw0UXXURHRwfr1q1j0aJFLFu2jMWLFwNs1hvPtkTuEoGbhsysOwcffDDLly/nj3/8I21tbYwcOZI99tiDDRs28NWvfpUFCxZQKBRYtmwZf/rTn9h11103uc/f/e53nH766TQ1NbHLLrswffp0Hn30UaZMmcKnP/1pNmzYwEc/+lEmT57M3nvvzYsvvsjnP/95TjjhBI4++uhtcNY5SgRyIjDrX3r4yz1Lp5xyCnfeeSevv/46p556KgC33HILbW1tPPbYYzQ3NzNu3Liaw09vjiOOOIIFCxbwi1/8grPPPpvzzz+fGTNm8MQTTzB//nyuvfZa5syZw/XXX781TqtHuekjKHUW+1WVZtaTU089ldtvv50777yTU045BUiGn37Xu95Fc3Mz9913H6+88krd+zv88MO544476OjooK2tjQULFnDYYYfxyiuvsMsuu3Duuedyzjnn8Pjjj7NixQqKxSInn3wy3/zmN3n88cezOs1O8lMjKDQlE64RmFkPJk6cyJo1a9h9993ZbbfdADjjjDP4yEc+wqRJk2hpadmsF8F87GMf46GHHuKggw5CEt/+9rfZddddufHGG7nyyitpbm5mhx124KabbmLZsmV86lOfolhM+jL/6Z/+KZNzrJbpMNRZ6O0w1Ev+6x72+/XZLDp6DpPfd0wGkZnZlvIw1FtHvxmGeltzH4GZWW25SQSlU/Xto2ZmneUmEchjDZn1C/5/dMv05vvLTSIojT7qpiGzvmvo0KGsXLnSyaCXIoKVK1cydOjQzdouP3cNlfsI3DRk1leNHTuW1tZW2traGh1KvzV06FDGjh27WdvkJhH4yWKzvq+5uZnx48c3OozcyU3TkPsIzMxqy1EiKJ2qE4GZWaXcJIJS01DRNQIzs05ylwgourPYzKxSbhKB0lMVTgRmZpUySwSSrpe0XNLibpZL0tWSlkp6UtIhWcWSHg/wTUNmZtWyrBH8CDi2h+XHAfumn/OAf80wFii4s9jMrJbMEkFELAD+3MMqJwE3ReJhYCdJu2UVj3AfgZlZLY3sI9gdeLVivjUt60LSeZIWSlrY2ycOVSi9mMaJwMysUr/oLI6I2RHREhEtY8aM6dU+3EdgZlZbIxPBMmCPivmxaVlGSoPOuUZgZlapkYlgLjAjvXtoKrA6Il7L7GjuLDYzqymzQeck3QYcCYyW1Ap8A2gGiIhrgXnA8cBSYB3wqaxigdKr64GiE4GZWaXMEkFEnL6J5QF8NqvjV1O5RuCmITOzSv2is3jr8OijZma15CYRFPyGMjOzmnKTCEqdxa4RmJl1lp9EUOY+AjOzSrlJBKUni900ZGbWWX4SgfsIzMxqyk0iKD9J4ERgZtZJbhJB6TmC8JPFZmad5CcRuGnIzKym3CSCQumdxb5ryMysk9wkgih3EbhGYGZWKTeJwE1DZma15S8RuLPYzKyTHCUCv7PYzKyWHCUCNw2ZmdWSm0RA+a4hJwIzs0q5SQQehtrMrLbcJAIKfjGNmVktuUkEpc5i+YEyM7NOcpcIXCEwM+ssR4nAL683M6slN4mgfNeQqwRmZp3kJhFsrBGYmVmlTH87SjpW0hJJSyXNrLF8T0n3SfqDpCclHZ9VLIVC6fZRNw2ZmVXKLBFIagJmAccBE4DTJU2oWu1rwJyIOBg4Dbgmq3hKbyjz7aNmZp1lWSM4DFgaES9GxDvA7cBJVesEsGM6PQL4Y1bBlF5eLz9ZbGbWSZaJYHfg1Yr51rSs0iXAmZJagXnA52vtSNJ5khZKWtjW1tarYFR+Z7GbhszMKjW6B/V04EcRMRY4HrhZNXp1I2J2RLRERMuYMWN6dSCV+wh6H6yZ2UCUZSJYBuxRMT82Lav0GWAOQEQ8BAwFRmcRjPyqSjOzmrJMBI8C+0oaL2kwSWfw3Kp1/gc4CkDS/iSJoHdtP5vgYajNzGrLLBFERDvwOWA+8CzJ3UFPS7pM0onpal8GzpX0BHAbcHZkdFtPoeBhqM3MahmU5c4jYh5JJ3Bl2cUV088A07KMoaRUI/Dto2ZmndVVI5C0l6QPptPbSRqebVgZcNOQmVlNm0wEks4F7gSuS4vGAv+RZVDZ8DDUZma11FMj+CxJ881fACLieeBdWQaVCQ86Z2ZWUz2J4O30yWAAJA2iX/a4urPYzKyWehLBA5K+Cmwn6UPAT4CfZRtWBtxHYGZWUz2JYCbJvf1PAX9LchfQ17IMKhNuGjIzq2mTt49GRBH4Yfrpv+SmITOzWjaZCCS9RI3fnhGxdyYRZc01AjOzTup5oKylYnoocAqwczbhZKsYwjUCM7PONtlHEBErKz7LIuIq4IRtENtWV0SuEZiZVamnaeiQitkCSQ0h06EpshII+X0EZmad1PML/Z8rptuBl4FPZBLNNuEagZlZpXruGvrAtghkWwjkPGBmVqXbRCDp/J42jIjvbv1wshUS4bGGzMw66alG0P9GGN2EIkKuEZiZddJtIoiIS7dlINuG8Ksqzcw6q+euoaEk7xaeSPIcAQAR8ekM48pEgG8fNTOrUs9YQzcDuwLHAA+QvI9gTZZBZSXwA2VmZtXqSQT7RMTXgTcj4kaSh8n+OtuwsuFEYGbWVT2JYEP6c5WkA4AR9McX01C6fdSJwMysUj0PlM2WNBL4OjAX2CGd7neSJ4udCMzMKtWTCG6IiA6S/oH+OeJoKnzXkJlZF/U0Db0kabako6TyoP79VrhGYGbWST2J4D3AvSQvsX9Z0g8kvb+enUs6VtISSUslzexmnU9IekbS05JurT/0zec+AjOzruoZa2gdMAeYk/YV/AtJM1FTT9tJagJmAR8CWoFHJc2NiGcq1tkXuBCYFhFvSMq0E9qJwMysq3pqBEiaLuka4DGSh8rqGX30MGBpRLwYEe8AtwMnVa1zLjArIt4AiIjldUfeC+4jMDPrqp4ni18G/kBSK7ggIt6sc9+7A69WzLfS9fmD/5Ue479IahiXRMR/1ojhPOA8gD333LPOw3cVKvgxAjOzKvXcNXRgRPwlw+PvCxxJ8sTyAkmTImJV5UoRMRuYDdDS0tLrX+XJ7aMdvY/WzGwAqudVlb1NAsuAPSrmx6ZllVqBuRGxISJeAv6bJDFkokgBuWnIzKyTuvoIeulRYF9J4yUNBk4jeSCt0n+Q1AaQNJqkqejFrAIqUnBnsZlZlcwSQUS0A58D5gPPAnMi4mlJl0k6MV1tPrBS0jPAfSR9ECszi0luGjIzq1ZPZ/EXgRtIRhz9N+BgYGZE/GpT20bEPGBeVdnFFdMBnJ9+MucagZlZV/XUCD6d9hMcDYwEPglcnmlUGQkKFFwjMDPrpJ5EUBpW4njg5oh4uqKsXwn5OQIzs2r1JILHJP2KJBHMlzScfvrbtEiTm4bMzKrU8xzBZ4DJwIsRsU7SzsCnsg0rG4Eo4KYhM7NK9dQI3gssiYhVks4EvgaszjasbIQKKPplZcbMLDP1JIJ/BdZJOgj4MvACcFOmUWUkfNeQmVkX9SSC9vQ2z5OAH0TELGB4tmFlIyQ/WWxmVqWePoI1ki4kuW30cEkFoDnbsLJRpImCm4bMzDqpp0ZwKvA2yfMEr5OMGXRlplFlJFSgn97wZGaWmXoGnXsduAUYIenDwPqI6Kd9BHKNwMysyiYTgaRPAL8HTiF5Ic0jkj6edWBZCBWQX0hgZtZJPX0EFwFTSm8PkzSG5B3Gd2YZWBaCggedMzOrUk8fQaHqFZIr69yu75HfR2BmVq2eGsF/SpoP3JbOn0rViKL9RdEPlJmZdbHJRBARF0g6GZiWFs2OiHuyDSsr7iMwM6tWT42AiLgLuCvjWDIXkoehNjOr0m0ikLQGav75LJJ3yuyYWVQZCTW5RmBmVqXbRBAR/XIYiZ6EO4vNzLron3f/9JofKDMzq5arROCmITOzrnKWCDz6qJlZtVwlAihQcCIwM+skV4kg1ETBL6YxM+sk00Qg6VhJSyQtlTSzh/VOlhSSWrKMBzcNmZl1kVkikNQEzAKOAyYAp0uaUGO94cAXgUeyiqUk5KYhM7NqWdYIDgOWRsSLEfEOcDvJ6y6r/SNwBbA+w1gAD0NtZlZLlolgd+DVivnWtKxM0iHAHhHxi552JOk8SQslLWxra+t9RK4RmJl10bDO4vTdx98FvrypdSNidkS0RETLmDFjtuCoTgRmZtWyTATLgD0q5semZSXDgQOA+yW9DEwF5mbZYey7hszMusoyETwK7CtpvKTBwGnA3NLCiFgdEaMjYlxEjAMeBk6MiIWZRSS5RmBmViWzRBAR7cDngPnAs8CciHha0mWSTszquD3yoHNmZl3U9T6C3oqIeVS9zSwiLu5m3SOzjAUgCk0UfNeQmVknuXqy2J3FZmZd5SsRyDUCM7NqOUsE7iw2M6uWr0RQSJqGwreQmpmV5SoRRNo0VHQeMDMry1UikAoUCNqLbh4yMyvJVyIoFCgoKHa4SmBmVpKrREChCYANHe0NDsTMrO/IVSJQmgg6NmxocCRmZn1HrhIBheRB6vYOJwIzs5JcJQI1NQPQ0e5EYGZWkqtEUKoRdGx4p8GBmJn1HblKBKUaQbHdncVmZiU5SwRpjaDdNQIzs5KcJYK0j8CdxWZmZflKBOU+AicCM7OSXCWCwqC0j8A1AjOzslwlApoGA1DscB+BmVlJrhJBU9pZXHTTkJlZWa4SQfn2UY81ZGZWlqtEUBiU1gjcR2BmVpavRFDuI3AiMDMryVkiSGoE4bGGzMzKMk0Eko6VtETSUkkzayw/X9Izkp6U9BtJe2UZj28fNTPrKrNEIKkJmAUcB0wATpc0oWq1PwAtEXEgcCfw7aziAWhKE0G4s9jMrCzLGsFhwNKIeDEi3gFuB06qXCEi7ouIdensw8DYDOMp9xFE0c8RmJmVZJkIdgderZhvTcu68xngl7UWSDpP0kJJC9va2nodULlpyKOPmpmV9YnOYklnAi3AlbWWR8TsiGiJiJYxY8b0+jhNzUkioOhEYGZWMijDfS8D9qiYH5uWdSLpg8BFwPSIeDvDeGgalDYNuY/AzKwsyxrBo8C+ksZLGgycBsytXEHSwcB1wIkRsTzDWABoSp8sxmMNmZmVZZYIIqId+BwwH3gWmBMRT0u6TNKJ6WpXAjsAP5G0SNLcbna3VQxq9l1DZmbVsmwaIiLmAfOqyi6umP5glsevNnjwEMDPEZiZVeoTncXbSnOaCKI9064IM7N+JVeJQM3Dkon29Y0NxMysD8lVIqBQ4B0GoQ1OBGZmJflKBMDbDEYdTgRmZiW5SwTvaAgFNw2ZmZXlLhFsoJmCawRmZmW5SwTvFIbQVPRdQ2ZmJblLBO0aQlOHE4GZWUnuEsGGwhAGuUZgZlaWu0TQURhCc7Zj25mZ9Su5SwTtTUNdIzAzq5C7RFBsGsLg8OijZmYluUsE0TyMIeHbR83MSnKXCIpDRjA81tJRjEaHYmbWJ+QuETB0JNvrbda+ua7RkZiZ9Qm5SwQaNhKAtatXNDgSM7O+IXeJoGn7JBGsW9XW4EjMzPqG3CWCwTvsDMD6NSsbHImZWd+Qu0Sw/YjRAKxbtbzBkZiZ9Q25SwSjxu4DwNsrX2lwJGZmfUPuEsH2I3fjLYZQWOVEYGYGOUwESPypaVeGrXUiMDODPCYC4M8jDmDvt56mvb290aGYmTVcpolA0rGSlkhaKmlmjeVDJN2RLn9E0rgs4ylp2udv2ElrefL+u7bF4czM+rRBWe1YUhMwC/gQ0Ao8KmluRDxTsdpngDciYh9JpwFXAKdmFVPJhKPOYNmjV7Lf777Ak4vvoGO70WjQEIpNQ6AwKP00gZqgUECl6aZBoKaN84UmUIEoNAFKPhJS8hMKyU8pmU3nhYjSelSuv7GMQtX2CKlQsa90G23cZ1JUqNhnId1fqjytNNx0O5QuVuf1RHm5VLV9l/U3rlcavKO0vBRT7WOo8zGqYi2t3+n7qoyhUJorbNyfNi5HlfurjKdzLKW9bLwWFUcqbDxvlb/3rl9r5/BVo2zjXqrLqHO9Wsfocpo1lpv1JLNEABwGLI2IFwEk3Q6cBFQmgpOAS9LpO4EfSFJEZDoQUPOQ7YgZP+WpO7/K7qsWs+MbaxjCBoZqQ5aHtQGqGGmySucDVf3sXA7a5LqVAvU4X3u76m22zn6jU3qqZxvVfezO++1dvJvapvZ+qnWzjSrn6znWll0D1ShbNv7jHH7WpTX2tGWyTAS7A69WzLcCf93dOhHRLmk1MArIfPyHsePfw9gL7gagWAzWt3ewur1IFDvoaN9AR7EdikU62tspdrQTxSLFYjvFjg1ERwdR7CCiA4rphyAiICKZLiY/iWJSThDFIkEggigCFCGCIN0uomLdKC8v77NieuN2RSLS/90iiPI2lPcFkB5wowgCUDnnpsvL/1qT5ZSWl9eLdDb5qU7rRaftqSzvUtZ5n5WxRNWxVN5vul5VDJRjoXPM5XOq/F8wXbfGsXved+f1qs+l06+7KP2n8vuvXFj7O924741EdA6/1q+T6BRRXduoxjbVW6jLZpXXshtd/oar/euvOpbKku5Sx6Z2K4pdC7usu4n4avwNmvy7ior56mtdK5Yax6n5ffY03zUljRize+0DbqEsE8FWI+k84DyAPffcc6vvv1AQwwYPgsGlku22+jHMzPqqLDuLlwF7VMyPTctqriNpEDAC6DL2Q0TMjoiWiGgZM2ZMRuGameVTlongUWBfSeMlDQZOA+ZWrTMXOCud/jjw26z7B8zMrLPMmobSNv/PAfOBJuD6iHha0mXAwoiYC/w7cLOkpcCfSZKFmZltQ5n2EUTEPGBeVdnFFdPrgVOyjMHMzHqWyyeLzcxsIycCM7OccyIwM8s5JwIzs5xTf7tbU1Ib0NsxpEezDZ5a7mN8zvngc86HLTnnvSKi5oNY/S4RbAlJCyOipdFxbEs+53zwOedDVufspiEzs5xzIjAzy7m8JYLZjQ6gAXzO+eBzzodMzjlXfQRmZtZV3moEZmZWxYnAzCzncpMIJB0raYmkpZJmNjqe3pK0h6T7JD0j6WlJX0zLd5b0a0nPpz9HpuWSdHV63k9KOqRiX2el6z8v6azujtlXSGqS9AdJP0/nx0t6JD23O9LhzpE0JJ1fmi4fV7GPC9PyJZKOacyZ1EfSTpLulPScpGclvXegX2dJ/zf9d71Y0m2Shg606yzpeknLJS2uKNtq11XSoZKeSre5Wqrj5dWRvh5xIH9IhsF+Adib5D1kTwATGh1XL89lN+CQdHo48N/ABODbwMy0fCZwRTp9PPBLkrfeTQUeSct3Bl5Mf45Mp0c2+vw2ce7nA7cCP0/n5wCnpdPXAn+fTv8f4Np0+jTgjnR6QnrthwDj038TTY0+rx7O90bgnHR6MLDTQL7OJK+ufQnYruL6nj3QrjNwBHAIsLiibKtdV+D36bpKtz1ukzE1+kvZRl/8e4H5FfMXAjFVLdIAAASbSURBVBc2Oq6tdG4/BT4ELAF2S8t2A5ak09cBp1esvyRdfjpwXUV5p/X62ofkDXe/Af4G+Hn6j3wFMKj6GpO8A+O96fSgdD1VX/fK9frah+RtfS+R3tBRff0G4nVm4zvMd06v28+BYwbidQbGVSWCrXJd02XPVZR3Wq+7T16ahkr/wEpa07J+La0KHww8AuwSEa+li14Hdkmnuzv3/vadXAX8A+U30jMKWBUR7el8Zfzlc0uXr07X70/nPB5oA25Im8P+TdL2DODrHBHLgO8A/wO8RnLdHmNgX+eSrXVdd0+nq8t7lJdEMOBI2gG4C/hSRPylclkkfwoMmPuCJX0YWB4RjzU6lm1oEEnzwb9GxMHAmyRNBmUD8DqPBE4iSYLvBrYHjm1oUA3QiOual0SwDNijYn5sWtYvSWomSQK3RMTdafGfJO2WLt8NWJ6Wd3fu/ek7mQacKOll4HaS5qF/AXaSVHrLXmX85XNLl48AVtK/zrkVaI2IR9L5O0kSw0C+zh8EXoqItojYANxNcu0H8nUu2VrXdVk6XV3eo7wkgkeBfdO7DwaTdCzNbXBMvZLeAfDvwLMR8d2KRXOB0p0DZ5H0HZTKZ6R3H0wFVqdV0PnA0ZJGpn+JHZ2W9TkRcWFEjI2IcSTX7rcRcQZwH/DxdLXqcy59Fx9P14+0/LT0bpPxwL4kHWt9TkS8Drwqab+06CjgGQbwdSZpEpoqaVj677x0zgP2OlfYKtc1XfYXSVPT73BGxb661+hOk23YOXM8yR02LwAXNTqeLTiP95NUG58EFqWf40naRn8DPA/cC+ycri9gVnreTwEtFfv6NLA0/Xyq0edW5/kfyca7hvYm+R98KfATYEhaPjSdX5ou37ti+4vS72IJddxN0eBznQwsTK/1f5DcHTKgrzNwKfAcsBi4meTOnwF1nYHbSPpANpDU/D6zNa8r0JJ+fy8AP6DqhoNaHw8xYWaWc3lpGjIzs244EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYbUOSjlQ6eqpZX+FEYGaWc04EZjVIOlPS7yUtknSdknchrJX0vXS8/N9IGpOuO1nSw+l48fdUjCW/j6R7JT0h6XFJf5XufgdtfM/ALXWNF2+WIScCsyqS9gdOBaZFxGSgAziDZBC0hRExEXgA+Ea6yU3AVyLiQJKnP0vltwCzIuIg4H0kT5NCMmLsl0jGzd+bZDwds4YZtOlVzHLnKOBQ4NH0j/XtSAYBKwJ3pOv8GLhb0ghgp4h4IC2/EfiJpOHA7hFxD0BErAdI9/f7iGhN5xeRjE3/u+xPy6w2JwKzrgTcGBEXdiqUvl61Xm/HZ3m7YroD/39oDeamIbOufgN8XNK7oPw+2b1I/n8pjYL5v4HfRcRq4A1Jh6flnwQeiIg1QKukj6b7GCJp2DY9C7M6+S8RsyoR8YykrwG/klQgGSXysyQvhzksXbacpB8BkmGDr01/0b8IfCot/yRwnaTL0n2csg1Pw6xuHn3UrE6S1kbEDo2Ow2xrc9OQmVnOuUZgZpZzrhGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnl3P8H2wtCa3xaRe8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y00VNhpv3ibM"
      },
      "source": [
        "### Task 1 - D: Inference\n",
        "\n",
        "We have defined, trained, and saved two models in `Task 1 - C`, as we can tell that the performance of Edge regression is better than Lasso regression. In this section, we will define a test function to calculate the MSE, RMSE, MAE based on the choosen model.\n",
        "\n",
        "As we know the formulas of the mentioned loss functions are: \n",
        "\n",
        "* $MAE = \\frac{1}{m}\\sum_{i=1}^{m}|y_i - \\hat y_i|$\n",
        "* $MSE = \\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat y_i)^2$\n",
        "* $RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(y_i - \\hat y_i)^2}$\n",
        "\n",
        "In PyTorch, we can define MSE by `torch.nn.MSEloss()`, define MAE by `torch.nn.L1Loss()`, according to the above foumlas, we can write RMSE manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "M6_Y__kg9aQD"
      },
      "source": [
        "def test_data(model_path, x_test, y_test, visualize=False):\n",
        "    \"\"\"\n",
        "    input the path of model, x_test, y_test, the function will print the mse, rmse, mae\n",
        "    based on the loaded model\n",
        "    \"\"\"\n",
        "    model = torch.load(model_path)\n",
        "    mse_loss = torch.nn.MSELoss()\n",
        "    mae_loss = torch.nn.L1Loss()\n",
        "    # rmse_loss = torch.sqrt(mse_loss)\n",
        "    predict = model(x_test)\n",
        "    mse = mse_loss(y_test, predict)     # calculate mse\n",
        "    mae = mae_loss(y_test, predict)     # calculate mae\n",
        "    rmse = torch.sqrt(mse_loss(y_test, predict))    # calculate rmse\n",
        "\n",
        "    print(f'mae:{mae.item():.4f}\\tmse: {mse.item():.4f}\\trmse:{rmse.item():.4f}')\n",
        "    return round(mae.item(), 4), round(mse.item(), 4), round(rmse.item(), 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4ptN9RrvkoV",
        "outputId": "52fe142e-1bc5-4152-b79d-eab3b24ebbcf"
      },
      "source": [
        "test_data('./EdgeLinear.pth', normed_x_test.cuda(0), normed_y_test.cuda(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae:0.0424\tmse: 0.0035\trmse:0.0593\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0424, 0.0035, 0.0593)"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2cSGP05HF9-"
      },
      "source": [
        "By here we can see in the EdgeLiear model, the error are: \n",
        "\n",
        "- $MAE = 0.0431$\n",
        "- $MSE = 0.0036$\n",
        "- $RMSE = 0.0597$\n",
        "\n",
        "Comparing with the MSE in trainining dataset, we can tell that the $MSE_{test}$ is not much larger than $MSE_{train}$, inversely, $MSE_{test}$ is smaller than $MSE_{train}$, therefore, we can say the generalization of the trained model is relatively eligible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o1TXXPcIGO-"
      },
      "source": [
        "### Task 1 - E: Feature Importance\n",
        "\n",
        "In this section, we will infer the most important features based on the given data and trained model. Our idea is that we will eliminate one of the features in the data set. Then we will run this feature eliminated feature on our trained model to check the error. Then we will count the difference between this error with the error which wasn't eliminated. The bigger difference is, the feature is more important. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQSGy4eSyZ0C"
      },
      "source": [
        "def feature_eliminate(test_data, feature_num):\n",
        "    \"\"\"\n",
        "    Feature number representation:\n",
        "        0 -- Variety\n",
        "        1 -- S1\n",
        "        2 -- S2\n",
        "        3 -- S3\n",
        "        4 -- S4\n",
        "        5 -- M1\n",
        "        6 -- M2\n",
        "        7 -- M3\n",
        "        8 -- W1\n",
        "        9 -- W2\n",
        "        10 -- W3\n",
        "        11 -- W4\n",
        "    \"\"\"\n",
        "    new_test = deepcopy(test_data)\n",
        "    new_test[..., feature_num] = 0\n",
        "    return new_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "628LlKFv0A2r"
      },
      "source": [
        "# eliminate the each features\n",
        "\n",
        "variety_x_test = feature_eliminate(normed_x_test, 0)\n",
        "\n",
        "s1_x_test = feature_eliminate(normed_x_test, 1)\n",
        "s2_x_test = feature_eliminate(normed_x_test, 2)\n",
        "s3_x_test = feature_eliminate(normed_x_test, 3)\n",
        "s4_x_test = feature_eliminate(normed_x_test, 4)\n",
        "\n",
        "m1_x_test = feature_eliminate(normed_x_test, 5)\n",
        "m2_x_test = feature_eliminate(normed_x_test, 6)\n",
        "m3_x_test = feature_eliminate(normed_x_test, 7)\n",
        "\n",
        "w1_x_test = feature_eliminate(normed_x_test, 8)\n",
        "w2_x_test = feature_eliminate(normed_x_test, 9)\n",
        "w3_x_test = feature_eliminate(normed_x_test, 10)\n",
        "w4_x_test = feature_eliminate(normed_x_test, 11)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3lTFvyl0yFt",
        "outputId": "3c8f7eb2-c7fe-401c-cec3-101f46b0e8df"
      },
      "source": [
        "# using the eliminated data, run on the trained model to count mae, mse, rmse.\n",
        "\n",
        "original_error = test_data('./EdgeLinear.pth', normed_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "\n",
        "errors = {}\n",
        "errors['Variety'] = test_data('EdgeLinear.pth', variety_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['S_1'] = test_data('EdgeLinear.pth', s1_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['S_2'] = test_data('EdgeLinear.pth', s2_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['S_3'] = test_data('EdgeLinear.pth', s3_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['S_4'] = test_data('EdgeLinear.pth', s4_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "\n",
        "errors['M_1'] = test_data('EdgeLinear.pth', m1_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['M_2'] = test_data('EdgeLinear.pth', m2_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['M_3'] = test_data('EdgeLinear.pth', m3_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "\n",
        "errors['W_1'] = test_data('EdgeLinear.pth', w1_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['W_2'] = test_data('EdgeLinear.pth', w2_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['W_3'] = test_data('EdgeLinear.pth', w3_x_test.cuda(0), normed_y_test.cuda(0))\n",
        "errors['W_4'] = test_data('EdgeLinear.pth', w4_x_test.cuda(0), normed_y_test.cuda(0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mae:0.0424\tmse: 0.0035\trmse:0.0593\n",
            "mae:0.0509\tmse: 0.0051\trmse:0.0717\n",
            "mae:0.0828\tmse: 0.0086\trmse:0.0928\n",
            "mae:0.0785\tmse: 0.0078\trmse:0.0885\n",
            "mae:0.0448\tmse: 0.0035\trmse:0.0595\n",
            "mae:0.0514\tmse: 0.0053\trmse:0.0730\n",
            "mae:0.1581\tmse: 0.0275\trmse:0.1659\n",
            "mae:0.0392\tmse: 0.0035\trmse:0.0589\n",
            "mae:0.1278\tmse: 0.0189\trmse:0.1376\n",
            "mae:0.0425\tmse: 0.0035\trmse:0.0593\n",
            "mae:0.0413\tmse: 0.0038\trmse:0.0614\n",
            "mae:0.0419\tmse: 0.0033\trmse:0.0572\n",
            "mae:0.0987\tmse: 0.0132\trmse:0.1148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpbHoqVP206Y"
      },
      "source": [
        "def importance_prs(original_err, other_errors):\n",
        "    \"\"\"count the difference based on mae, mse, rmse\"\"\"\n",
        "    importance_mae = {}\n",
        "    importance_mse = {}   \n",
        "    importance_rmse = {}\n",
        "    for k, v in other_errors.items():\n",
        "        mae_dff = round(abs(original_error[0] - v[0]), 4)\n",
        "        importance_mae[k] = mae_dff\n",
        "\n",
        "        mse_dff = round(abs(original_error[1] - v[1]), 4)\n",
        "        importance_mse[k] = mse_dff\n",
        "\n",
        "        rmse_dff = round(abs(original_error[2] - v[2]), 4)\n",
        "        importance_rmse[k] = rmse_dff\n",
        "    return importance_mae,importance_mse,importance_rmse"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S86SBOy25rzI"
      },
      "source": [
        "importances = importance_prs(original_error, errors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6IIHLEg6ral"
      },
      "source": [
        "def importance_evl(importances, matrix = 'mse'):\n",
        "    \"\"\"print the importance from unimportant to important sequence based on the pointed matrix\"\"\"\n",
        "    if matrix == 'mae':\n",
        "        print(sorted(importances[0].items(), key=lambda item:item[1]))\n",
        "    elif matrix == 'mse':\n",
        "        print(sorted(importances[1].items(), key=lambda item:item[1]))\n",
        "    else:\n",
        "        print(sorted(importances[2].items(), key=lambda item:item[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLtEtVB-9gUw",
        "outputId": "97d341b6-02b4-4ca1-dbff-8deab0370766"
      },
      "source": [
        "importance_evl(importances, 'mae')\n",
        "importance_evl(importances, 'mse')\n",
        "importance_evl(importances, 'rmse')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('W_1', 0.0001), ('W_3', 0.0005), ('W_2', 0.0011), ('S_3', 0.0024), ('M_2', 0.0032), ('Variety', 0.0085), ('S_4', 0.009), ('S_2', 0.0361), ('S_1', 0.0404), ('W_4', 0.0563), ('M_3', 0.0854), ('M_1', 0.1157)]\n",
            "[('S_3', 0.0), ('M_2', 0.0), ('W_1', 0.0), ('W_3', 0.0002), ('W_2', 0.0003), ('Variety', 0.0016), ('S_4', 0.0018), ('S_2', 0.0043), ('S_1', 0.0051), ('W_4', 0.0097), ('M_3', 0.0154), ('M_1', 0.024)]\n",
            "[('W_1', 0.0), ('S_3', 0.0002), ('M_2', 0.0004), ('W_2', 0.0021), ('W_3', 0.0021), ('Variety', 0.0124), ('S_4', 0.0137), ('S_2', 0.0292), ('S_1', 0.0335), ('W_4', 0.0555), ('M_3', 0.0783), ('M_1', 0.1066)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vLt70MAiCY"
      },
      "source": [
        "From above result, we can tell that to different matrix, the feature importance is different. If we choose MAE as our matrix, then the importance of data features is: \n",
        "\n",
        "$W_1<W_3<W_2<S_3<M_2<Variety<S_4<S_2<S_1<W_4<M_3<M_1$\n",
        "\n",
        "If we choose MSE as our matrix, then the feature importance is: \n",
        "\n",
        "$S_3<M_2<W_1<W_3<W_2<Variety<S_4<S_2<S_1<W_4<M_3<M_1$\n",
        "\n",
        "If we choose RMSE as our matrix, then the feature immportance is: \n",
        "$W_1<S_3<M_2<W_2<W_3<Variety<S_4<S_2<S_1<W_4<M_3<M_1$\n",
        "\n",
        "Especially, we can tell no matter what matrix we choose, the most 7 important features are exactly same, as well as the sequence. The rest features have barely affection to the result so they are not important features to the data. Therefore, we can conclude that these 7 features are important for the data. The importance from high to low is $M_1 > M_3 > W_4 > S_1 > S_2 > S_4 > Variety$. "
      ]
    }
  ]
}